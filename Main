import requests
import time
from urllib.robotparser import RobotFileParser
from urllib.parse import urljoin, urlparse
import threading
from bs4 import BeautifulSoup

class PoliteCrawler:
    def __init__(self, base_url, delay=1):
        self.base_url = base_url
        self.delay = delay
        self.robots_txt = self.init_robots_txt()
        self.visited_urls = set()

    def init_robots_txt(self):
        robots_url = urljoin(self.base_url, 'robots.txt')
        rp = RobotFileParser()
        rp.set_url(robots_url)
        rp.read()
        return rp

    def can_fetch(self, url):
        return self.robots_txt.can_fetch("*", url)

    def parse_html(self, html):
        soup = BeautifulSoup(html, 'html.parser')
        links = set()
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href.startswith('http') or href.startswith('/'):
                full_url = urljoin(self.base_url, href)
                links.add(full_url)
        return links

    def crawl(self, url, depth=1):
        if depth == 0 or not self.can_fetch(url) or url in self.visited_urls:
            return

        self.visited_urls.add(url)

        try:
            response = requests.get(url, timeout=5)
            response.raise_for_status()

            links = self.parse_html(response.text)

            for next_url in links:
                if next_url not in self.visited_urls:
                    threading.Thread(target=self.crawl, args=(next_url, depth-1)).start()

        except requests.exceptions.RequestException as e:
            print(f"Error fetching {url}: {e}")

        time.sleep(self.delay)

    def start(self, paths=['/'], depth=1):
        for path in paths:
            full_url = urljoin(self.base_url, path)
            threading.Thread(target=self.crawl, args=(full_url, depth)).start()

if __name__ == "__main__":
    crawler = PoliteCrawler("http://example.com", delay=1)
    crawler.start(paths=['/'], depth=2)
